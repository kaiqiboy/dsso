{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8909968f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "import os\n",
    "import sklearn\n",
    "import pickle\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from sklearn.preprocessing import normalize\n",
    "from random import randint\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error as mape\n",
    "import glob\n",
    "use_gpu = torch.cuda.is_available()\n",
    "\n",
    "device = 'cuda' if use_gpu else 'cpu'\n",
    "\n",
    "use_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cda9f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8\n",
    "seed = 17\n",
    "torch.use_deterministic_algorithms(True)\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(seed)\n",
    "if use_gpu:\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfa2101",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'encoded_imdb.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef2c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read parsed plans\n",
    "plans = pickle.load(open(data_dir, 'rb'))\n",
    "len(plans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c0113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and noralize times\n",
    "times = [float(t[1]) for t in plans.values()]\n",
    "time_max = max(times)\n",
    "times = [i if i > 0 else int(1.2 * time_max) for i in times]\n",
    "\n",
    "time_min = min(times)\n",
    "time_max = max(times)\n",
    "\n",
    "print(\"Total number of times: {}\".format(len(times)))\n",
    "print(\"Min time: {}, max time: {}\".format(time_min, time_max))\\\n",
    "\n",
    "times = [((i-time_min)/(time_max-time_min), idx) for (idx,i) in enumerate(times)] # normalize to [0, 1], now times is [time, idx] for error analysis\n",
    "\n",
    "for k in plans.keys():\n",
    "    if (plans[k])[1] > 0:\n",
    "        t = plans[k][1]\n",
    "    else:\n",
    "        t =  int(1.2 * time_max)\n",
    "    plans[k] = (plans[k][0], (t-time_min) / (time_max-time_min), plans[k][2])\n",
    "len(plans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a4ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDS\n",
    "from collections import Counter \n",
    "from scipy.ndimage import convolve1d\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "\n",
    "def get_lds_kernel_window(kernel, ks, sigma):\n",
    "    assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "    half_ks = (ks - 1) // 2\n",
    "    if kernel == 'gaussian':\n",
    "        base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "        kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / max(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "    elif kernel == 'triang':\n",
    "        kernel_window = triang(ks)\n",
    "    else:\n",
    "        laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "        kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / max(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "    return kernel_window\n",
    "\n",
    "def get_bin_idx(label, g):\n",
    "    return int(label / g)\n",
    "\n",
    "labels = [t[0] for t in times]\n",
    "bin_index_per_label = [get_bin_idx(label, 0.1) for label in labels]\n",
    "\n",
    "# calculate empirical (original) label distribution: [Nb,]\n",
    "# \"Nb\" is the number of bins\n",
    "Nb = max(bin_index_per_label) + 1\n",
    "num_samples_of_bins = dict(Counter(bin_index_per_label))\n",
    "emp_label_dist = [num_samples_of_bins.get(i, 0) for i in range(Nb)]\n",
    "\n",
    "# lds_kernel_window: [ks,], here for example, we use gaussian, ks=5, sigma=2\n",
    "lds_kernel_window = get_lds_kernel_window(kernel='gaussian', ks=5, sigma=2)\n",
    "# calculate effective label distribution: [Nb,]\n",
    "eff_label_dist = convolve1d(np.array(emp_label_dist), weights=lds_kernel_window, mode='constant')\n",
    "eff_num_per_label = [eff_label_dist[bin_idx] for bin_idx in bin_index_per_label]\n",
    "weights = [np.float32(1 / x) for x in eff_num_per_label]\n",
    "weights = np.array(weights) / np.mean(np.array(weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7a5a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and normalize resources (not really in use)\n",
    "resources = [(t[2]['n_executor'], t[2]['g_mem'], t[2]['n_core'], t[2]['n_worker']) for t in plans.values()]\n",
    "resources = np.array([np.array(x) for x in resources])\n",
    "normalized_resources = normalize(resources, axis=0, norm='max')\n",
    "normalized_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4a9484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad plans and convert it to list\n",
    "lens = [i[0].shape[0] for i in plans.values()]\n",
    "max_len = 57\n",
    "print('max length: ' + str(max_len))\n",
    "plans_list = []\n",
    "for i, k in enumerate(plans.keys()):\n",
    "    l = plans[k][0].shape[0]\n",
    "    padded = np.pad(plans[k][0], ((0, max_len-l),(0,0)), \"constant\", constant_values = (0))\n",
    "    plans_list.append((k[0], k[1], k[2], padded, plans[k][1], l/max_len, normalized_resources[i], weights[i]))\n",
    "\n",
    "print(len(plans_list))\n",
    "plans_list[0]\n",
    "print(plans_list[0][3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9fee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually split the data according to query idx\n",
    "\n",
    "plans_train = [] \n",
    "for id, i in enumerate(plans_list[:int(0.8*len(plans_list))]):\n",
    "    plans_train.append((i[0], i[1], i[2], i[3], i[4], i[5], i[6], i[7], id))\n",
    "plans_val = []\n",
    "for id, i in enumerate(plans_list[int(0.8*len(plans_list)):int(0.9*len(plans_list))]):\n",
    "    plans_val.append((i[0], i[1], i[2], i[3], i[4], i[5], i[6], i[7], id))\n",
    "plans_test = []\n",
    "for id, i in enumerate(plans_list[int(0.9*len(plans_list)):]):\n",
    "    plans_test.append((i[0], i[1], i[2], i[3], i[4], i[5], i[6], i[7], id))\n",
    "print(len(plans_train), len(plans_val), len(plans_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e70f889",
   "metadata": {},
   "outputs": [],
   "source": [
    "plans_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f52dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define custom dataset\n",
    "class QueryDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        (q_id, p_id, group_id, plans, times, l, resources, weight, i) = self.data[idx]\n",
    "        \n",
    "        return plans, times, q_id, int(p_id), l, resources, weight, group_id, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create manual data loader\n",
    "batch_size = 32 \n",
    "\n",
    "train_data = QueryDataset(plans_train)\n",
    "val_data = QueryDataset(plans_val)\n",
    "test_data = QueryDataset(plans_test)\n",
    "\n",
    "# generate weights for train_data\n",
    "num_bins = 5\n",
    "train_times= [i[4] for i in train_data.data]\n",
    "counts, edges = np.histogram(train_times, num_bins)\n",
    "bin_weights = [1/x for x in counts]\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "sample_weights = []\n",
    "for i in train_times:\n",
    "    sample_weights.append(bin_weights[min(num_bins - 1, int(i/(1/num_bins)))])\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights))\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False\n",
    ")\n",
    "plans, times, q_id, p_id, l, resources, weights, group_id, idx = next(iter(train_dataloader))\n",
    "print(plans.shape) #(batch_size, num_plans, num_operators, dim)\n",
    "print(times.shape)\n",
    "print(p_id.shape)\n",
    "print(l.shape)\n",
    "print(resources.shape)\n",
    "\n",
    "print(len(train_dataloader), len(val_dataloader), len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b396e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## FDS\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal.windows import triang\n",
    "\n",
    "def calibrate_mean_var(matrix, m1, v1, m2, v2, clip_min=0.1, clip_max=10):\n",
    "    if torch.sum(v1) < 1e-10:\n",
    "        return matrix\n",
    "    if (v1 == 0.).any():\n",
    "        valid = (v1 != 0.)\n",
    "        factor = torch.clamp(v2[valid] / v1[valid], clip_min, clip_max)\n",
    "        matrix[:, valid] = (matrix[:, valid] - m1[valid]) * torch.sqrt(factor) + m2[valid]\n",
    "        return matrix\n",
    "\n",
    "    factor = torch.clamp(v2 / v1, clip_min, clip_max)\n",
    "    return (matrix - m1) * torch.sqrt(factor) + m2\n",
    "\n",
    "class FDS(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_dim, bucket_num=100, bucket_start=0, start_update=0, start_smooth=1,\n",
    "                 kernel='gaussian', ks=5, sigma=2, momentum=0.9):\n",
    "        super(FDS, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.bucket_num = bucket_num\n",
    "        self.bucket_start = bucket_start\n",
    "        self.kernel_window = self._get_kernel_window(kernel, ks, sigma)\n",
    "        self.half_ks = (ks - 1) // 2\n",
    "        self.momentum = momentum\n",
    "        self.start_update = start_update\n",
    "        self.start_smooth = start_smooth\n",
    "\n",
    "        self.register_buffer('epoch', torch.zeros(1).fill_(start_update))\n",
    "        self.register_buffer('running_mean', torch.zeros(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('running_var', torch.ones(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('running_mean_last_epoch', torch.zeros(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('running_var_last_epoch', torch.ones(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('smoothed_mean_last_epoch', torch.zeros(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('smoothed_var_last_epoch', torch.ones(bucket_num - bucket_start, feature_dim))\n",
    "        self.register_buffer('num_samples_tracked', torch.zeros(bucket_num - bucket_start))\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_kernel_window(kernel, ks, sigma):\n",
    "        assert kernel in ['gaussian', 'triang', 'laplace']\n",
    "        half_ks = (ks - 1) // 2\n",
    "        if kernel == 'gaussian':\n",
    "            base_kernel = [0.] * half_ks + [1.] + [0.] * half_ks\n",
    "            base_kernel = np.array(base_kernel, dtype=np.float32)\n",
    "            kernel_window = gaussian_filter1d(base_kernel, sigma=sigma) / sum(gaussian_filter1d(base_kernel, sigma=sigma))\n",
    "        elif kernel == 'triang':\n",
    "            kernel_window = triang(ks) / sum(triang(ks))\n",
    "        else:\n",
    "            laplace = lambda x: np.exp(-abs(x) / sigma) / (2. * sigma)\n",
    "            kernel_window = list(map(laplace, np.arange(-half_ks, half_ks + 1))) / sum(map(laplace, np.arange(-half_ks, half_ks + 1)))\n",
    "\n",
    "        print(f'Using FDS: [{kernel.upper()}] ({ks}/{sigma})')\n",
    "        return torch.tensor(kernel_window, dtype=torch.float32).cuda()\n",
    "\n",
    "    def _update_last_epoch_stats(self):\n",
    "        self.running_mean_last_epoch = self.running_mean\n",
    "        self.running_var_last_epoch = self.running_var\n",
    "\n",
    "        self.smoothed_mean_last_epoch = F.conv1d(\n",
    "            input=F.pad(self.running_mean_last_epoch.unsqueeze(1).permute(2, 1, 0),\n",
    "                        pad=(self.half_ks, self.half_ks), mode='reflect'),\n",
    "            weight=self.kernel_window.view(1, 1, -1), padding=0\n",
    "        ).permute(2, 1, 0).squeeze(1)\n",
    "        self.smoothed_var_last_epoch = F.conv1d(\n",
    "            input=F.pad(self.running_var_last_epoch.unsqueeze(1).permute(2, 1, 0),\n",
    "                        pad=(self.half_ks, self.half_ks), mode='reflect'),\n",
    "            weight=self.kernel_window.view(1, 1, -1), padding=0\n",
    "        ).permute(2, 1, 0).squeeze(1)\n",
    "\n",
    "    def reset(self):\n",
    "        self.running_mean.zero_()\n",
    "        self.running_var.fill_(1)\n",
    "        self.running_mean_last_epoch.zero_()\n",
    "        self.running_var_last_epoch.fill_(1)\n",
    "        self.smoothed_mean_last_epoch.zero_()\n",
    "        self.smoothed_var_last_epoch.fill_(1)\n",
    "        self.num_samples_tracked.zero_()\n",
    "\n",
    "    def update_last_epoch_stats(self, epoch):\n",
    "        if epoch == self.epoch + 1:\n",
    "            self.epoch += 1\n",
    "            self._update_last_epoch_stats()\n",
    "            print(f\"Updated smoothed statistics on Epoch [{epoch}]!\")\n",
    "\n",
    "    def update_running_stats(self, features, labels, epoch):\n",
    "        if epoch < self.epoch:\n",
    "            return\n",
    "\n",
    "        assert self.feature_dim == features.size(1), \"Input feature dimension is not aligned!\"\n",
    "        assert features.size(0) == labels.size(0), \"Dimensions of features and labels are not aligned!\"\n",
    "\n",
    "        for label in torch.unique(labels):\n",
    "            if label > self.bucket_num - 1 or label < self.bucket_start:\n",
    "                continue\n",
    "            elif label == self.bucket_start:\n",
    "                curr_feats = features[labels <= label]\n",
    "            elif label == self.bucket_num - 1:\n",
    "                curr_feats = features[labels >= label]\n",
    "            else:\n",
    "                curr_feats = features[labels == label]\n",
    "            curr_num_sample = curr_feats.size(0)\n",
    "            curr_mean = torch.mean(curr_feats, 0)\n",
    "            curr_var = torch.var(curr_feats, 0, unbiased=True if curr_feats.size(0) != 1 else False)\n",
    "\n",
    "            self.num_samples_tracked[int(label - self.bucket_start)] += curr_num_sample\n",
    "            factor = self.momentum if self.momentum is not None else \\\n",
    "                (1 - curr_num_sample / float(self.num_samples_tracked[int(label - self.bucket_start)]))\n",
    "            factor = 0 if epoch == self.start_update else factor\n",
    "            self.running_mean[int(label - self.bucket_start)] = \\\n",
    "                (1 - factor) * curr_mean + factor * self.running_mean[int(label - self.bucket_start)]\n",
    "            self.running_var[int(label - self.bucket_start)] = \\\n",
    "                (1 - factor) * curr_var + factor * self.running_var[int(label - self.bucket_start)]\n",
    "\n",
    "        print(f\"Updated running statistics with Epoch [{epoch}] features!\")\n",
    "\n",
    "    def smooth(self, features, labels, epoch):\n",
    "        if epoch < self.start_smooth:\n",
    "            return features\n",
    "\n",
    "        labels = labels.squeeze(1)\n",
    "        for label in torch.unique(labels):\n",
    "            if label > self.bucket_num - 1 or label < self.bucket_start:\n",
    "                continue\n",
    "            elif label == self.bucket_start:\n",
    "                features[labels <= label] = calibrate_mean_var(\n",
    "                    features[labels <= label],\n",
    "                    self.running_mean_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.running_var_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])\n",
    "            elif label == self.bucket_num - 1:\n",
    "                features[labels >= label] = calibrate_mean_var(\n",
    "                    features[labels >= label],\n",
    "                    self.running_mean_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.running_var_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])\n",
    "            else:\n",
    "                features[labels == label] = calibrate_mean_var(\n",
    "                    features[labels == label],\n",
    "                    self.running_mean_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.running_var_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.smoothed_mean_last_epoch[int(label - self.bucket_start)],\n",
    "                    self.smoothed_var_last_epoch[int(label - self.bucket_start)])\n",
    "        return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60436145",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LSTM cost estimation\n",
    "config = dict(feature_dim=257, start_update=0, start_smooth=20, kernel='gaussian', ks=5, sigma=2)\n",
    "\n",
    "class LSTMNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2, auxi_len=5):\n",
    "        super(LSTMNet, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.auxi_len = auxi_len\n",
    "        self.T = nn.Parameter(torch.randn(1, input_dim, device=device), requires_grad=True) # to adjust the attention dimension\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
    "        self.fc1 = nn.Linear(output_dim + 1, int(output_dim/2))\n",
    "        self.fc2 = nn.Linear(int(output_dim/2), int(output_dim/4))\n",
    "        self.fc3 = nn.Linear(int(output_dim/4), 1)\n",
    "        self.bn = nn.BatchNorm1d(128)\n",
    "        self.FDS = FDS(**config)\n",
    "\n",
    "    def forward(self, x, epoch=0, labels = None):\n",
    "        \n",
    "        lens_n_resource = x[:, -1, :self.auxi_len].reshape(-1, self.auxi_len).float()\n",
    "        lens = lens_n_resource[:, 0].unsqueeze(1)\n",
    "        x = x[:, :-1, :].float()\n",
    "        h = self.init_hidden(x.shape[0])\n",
    "        out, h = self.lstm(x, h)\n",
    "\n",
    "        h = torch.mean(out, axis = 1)\n",
    "        h = torch.cat((h, lens), 1).float()\n",
    "        if  epoch >= config['start_smooth']:\n",
    "            h = self.FDS.smooth(h, labels, epoch)\n",
    "        m = nn.LeakyReLU(0.1) \n",
    "        x = self.fc1(h)\n",
    "        x = m(x)\n",
    "        out1 =m(self.fc2(x))\n",
    "        out2 = self.fc3(out1)\n",
    "        return out2, out1\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330ebef",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = LSTMNet(192, 256, 256, 2, auxi_len=5)\n",
    "\n",
    "if(use_gpu):\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0eb979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "scheduler = MultiStepLR(optimizer, milestones=[20,40,60,100,120], gamma=0.1)\n",
    "\n",
    "if(use_gpu):\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "def unnormalize(vecs, mini, maxi):\n",
    "    return vecs * (maxi - mini) + mini\n",
    "    \n",
    "def qerror_loss(preds, targets, mini, maxi):\n",
    "    qerror = []\n",
    "    preds = unnormalize(preds, mini, maxi)\n",
    "    targets = unnormalize(targets, mini, maxi)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i]/targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i]/preds[i])\n",
    "\n",
    "    return torch.mean(torch.cat(qerror))\n",
    "\n",
    "def weighted_qerror_loss(preds, targets, mini, maxi, weights):\n",
    "    qerror = []\n",
    "    preds = unnormalize(preds, mini, maxi)\n",
    "    targets = unnormalize(targets, mini, maxi)\n",
    "\n",
    "    for i in range(len(targets)):\n",
    "        if (preds[i] > targets[i]).cpu().data.numpy()[0]:\n",
    "            qerror.append(preds[i]/targets[i])\n",
    "        else:\n",
    "            qerror.append(targets[i]/preds[i])\n",
    "    \n",
    "    return torch.mean(torch.cat(qerror) * weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12a240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation\n",
    "\n",
    "def qerror(y_true, y_pred, percentage = 1.0):\n",
    "    q= []\n",
    "    for i in range(len(y_true)):\n",
    "        if(y_true[i] > y_pred[i]):\n",
    "            q.append(y_true[i] / y_pred[i])\n",
    "        else:\n",
    "            q.append(y_pred[i] / y_true[i])\n",
    "    q.sort()\n",
    "    if percentage == 1.0:\n",
    "        return np.array(q).sum() / len(q)\n",
    "    else: return q[int(percentage*len(q))]\n",
    "\n",
    "def evaluate(net, val_dataloader):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    y_qid = []\n",
    "    y_pid = []\n",
    "    group_ids = []\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        plans, times, q_id, p_id, lens, resources, _, group_id, idx  = data\n",
    "        lens_n_resources1 = torch.hstack((lens.reshape((-1, 1)), resources))\n",
    "        lens_n_resources_padded1 = lens_n_resources1.reshape((-1, 1, 5))\n",
    "        lens_n_resources_padded1 = F.pad(lens_n_resources_padded1, (0, plans.shape[-1]-5), 'constant', 0)\n",
    "        inputs1 = torch.cat((plans, lens_n_resources_padded1), 1)\n",
    "\n",
    "        labels1 = times.reshape((-1, 1)).to(torch.float32)\n",
    "        \n",
    "        if (use_gpu):\n",
    "            inputs1, labels1 = inputs1.cuda(), labels1.cuda()\n",
    "        \n",
    "        outputs = net.forward(inputs1, 0)\n",
    "\n",
    "        y_true.append(times.cpu().detach().numpy())\n",
    "        y_pred.append(outputs[0].cpu().detach().numpy())\n",
    "        # y_qid.append(q_id.cpu().detach().numpy())\n",
    "        y_pid.append(p_id.cpu().detach().numpy())\n",
    "\n",
    "    y_pred = np.concatenate(y_pred,axis=0).squeeze()\n",
    "    y_true = np.concatenate(y_true,axis=0).squeeze()\n",
    "    # y_qid = np.concatenate(y_qid,axis=0).squeeze()\n",
    "    y_pid = np.concatenate(y_pid,axis=0).squeeze()\n",
    "\n",
    "    y_pred = [(i*(time_max - time_min)+time_min)/1000 for i in y_pred]\n",
    "    y_true = [(i*(time_max - time_min)+time_min)/1000 for i in y_true]\n",
    "\n",
    "    try:\n",
    "        return (qerror(y_true, y_pred), mape(y_true, y_pred))\n",
    "    except:\n",
    "        return (0,0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc821b",
   "metadata": {},
   "outputs": [],
   "source": [
    "old = glob.glob(\"saved_models_imdb/epoch*\")\n",
    "for i in old:\n",
    "    os.remove(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329ace6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "current_best_mape = 10\n",
    "\n",
    "for epoch in range(150): \n",
    "    running_loss = 0.0\n",
    "    \n",
    "\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        plans, times, q_id, p_id, lens, resources, weights, _, idx = data\n",
    "        lens_n_resources1 = torch.hstack((lens.reshape((-1, 1)), resources))\n",
    "        lens_n_resources_padded1 = lens_n_resources1.reshape((-1, 1, 5))\n",
    "        lens_n_resources_padded1 = F.pad(lens_n_resources_padded1, (0, plans.shape[-1]-5), 'constant', 0)\n",
    "        inputs1 = torch.cat((plans, lens_n_resources_padded1), 1)\n",
    "        labels1 = times.reshape((-1, 1)).to(torch.float32)\n",
    "\n",
    "        if (use_gpu):\n",
    "            inputs1, labels1 = inputs1.cuda(), labels1.cuda()\n",
    "            weights = weights.cuda()\n",
    "\n",
    "        outputs = net.forward(inputs1, epoch, labels1)\n",
    "        optimizer.zero_grad()\n",
    "        if epoch > 50:\n",
    "            loss1 = weighted_qerror_loss(outputs[0], labels1, time_min, time_max, weights)\n",
    "        else:\n",
    "            loss1 = qerror_loss(outputs[0], labels1, time_min, time_max)\n",
    "        mse = nn.MSELoss()\n",
    "        loss = loss1\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    val_loss, val_mape = evaluate(net, val_dataloader)\n",
    "    \n",
    "    if val_mape < current_best_mape:\n",
    "        torch.save(net.state_dict(), f\"saved_models_imdb/epoch_{epoch+1}_{running_loss / len(train_dataloader):.5f}_{val_loss:.5f}_{val_mape:.5f}.pt\")\n",
    "        current_best_mape = val_mape\n",
    "        \n",
    "    print(f'[Epoch {epoch + 1}] train loss: {running_loss / 2 / len(train_dataloader):.5f} val loss: {val_loss:.5f} val mape: {val_mape:.5f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# clean saved models\n",
    "models = glob.glob(\"saved_models_imdb/epoch*\")\n",
    "models = [((i.split('_')[-1][:-3]), i) for i in models]\n",
    "models.sort()\n",
    "for m in models[3:]:\n",
    "    os.remove(m[1])\n",
    "print(f'The best model is {models[0][1]}')\n",
    "os.system(\"cp {} saved_models_imdb/model-a.pt\".format(models[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43a2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate\n",
    "import scipy\n",
    "net.load_state_dict(torch.load(models[0][1]))\n",
    "net.to('cuda')\n",
    "net.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_qid = []\n",
    "y_pid = []\n",
    "group_ids = []\n",
    "def gm(y_true, y_pred):\n",
    "    e = np.abs(np.array(y_true) - np.array(y_pred))\n",
    "    return np.prod(e)**(1/len(e))\n",
    "for i, data in enumerate(test_dataloader, 0):\n",
    "    plans, times, q_id, p_id, lens, resources, weights, group_id, idx = data\n",
    "    lens_n_resources1 = torch.hstack((lens.reshape((-1, 1)), resources, group_id.reshape((-1, 1))))\n",
    "    lens_n_resources_padded1 = lens_n_resources1.reshape((-1, 1, 6))\n",
    "    lens_n_resources_padded1 = F.pad(lens_n_resources_padded1, (0, plans.shape[-1]-6), 'constant', 0)\n",
    "    inputs1 = torch.cat((plans, lens_n_resources_padded1), 1)\n",
    "    labels1 = times.reshape((-1, 1)).to(torch.float32)\n",
    "    \n",
    "    if (use_gpu):\n",
    "        inputs1, labels1 = inputs1.cuda(), labels1.cuda()\n",
    "    \n",
    "    outputs = net.forward(inputs1, 0)\n",
    "\n",
    "    y_true.append(times.cpu().detach().numpy())\n",
    "    y_pred.append(outputs[0].cpu().detach().numpy())\n",
    "\n",
    "y_pred = np.concatenate(y_pred,axis=0).squeeze()\n",
    "y_true = np.concatenate(y_true,axis=0).squeeze()\n",
    "\n",
    "y_pred = [(i*(time_max - time_min)+time_min)/1000 for i in y_pred]\n",
    "y_true = [(i*(time_max - time_min)+time_min)/1000 for i in y_true]\n",
    "\n",
    "print(f'mape: {mape(y_true, y_pred)}')\n",
    "print(f'mse: {mean_squared_error(y_true, y_pred)}')\n",
    "print(f'r2: {r2_score(y_true, y_pred)}')\n",
    "print(f'pearson corr: {scipy.stats.pearsonr(y_true, y_pred)}')\n",
    "\n",
    "print(f'qerror 50%: {qerror(y_true, y_pred, 0.5)}')\n",
    "print(f'qerror 90%: {qerror(y_true, y_pred, 0.9)}')\n",
    "print(f'qerror: {qerror(y_true, y_pred)}')\n",
    "print(f'gm: {gm(y_true, y_pred)}')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(len(y_true))\n",
    "zipped = zip(y_true, y_pred)\n",
    "zipped =list(zipped)\n",
    "zipped = sorted(zipped, key=lambda x: x[0])\n",
    "negs = 0.0\n",
    "for i, j in zipped:\n",
    "    if (i > j): negs += i-j\n",
    "print(f'neg: {negs/len(y_true)}')\n",
    "\n",
    "plt.scatter(x,[i[0] for i in zipped], color=\"black\",alpha=0.1, label='ground truth')\n",
    "plt.scatter(x,[i[1] for i in zipped], color=\"red\", alpha=0.1, label='prediction')\n",
    "ax = plt.gca()\n",
    "plt.legend()\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "96d7d9c4850cad55e90ffbd2129f1103be678d0d036c41e3b7c74e0c2bc67127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
