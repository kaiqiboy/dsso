{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, re\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import normalize\n",
    "import pickle\n",
    "import math\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_table_metadata(file_name=\"imdb-metadata.json\"):\n",
    "    f = open(file_name)\n",
    "    columns_all = []\n",
    "    tables_all = []\n",
    "    for line in f:\n",
    "        metadata = json.loads(line)\n",
    "        table_name = metadata['name']\n",
    "        columns = metadata['columns']\n",
    "        tables_all.append((table_name, metadata['length'], metadata['size']))\n",
    "        for k in columns.keys():\n",
    "            columns_all.append((table_name + '.' + k, columns[k]))\n",
    "\n",
    "    # normalize columns\n",
    "    numerical = []\n",
    "    minValues = []\n",
    "    maxValues = []\n",
    "    nullss = []\n",
    "    distincts = []\n",
    "    for _,i in columns_all:\n",
    "        numerical.append(i['numerical'])\n",
    "        minValues.append(i['minValue'])\n",
    "        maxValues.append(i['maxValue'])\n",
    "        nullss.append(i['nulls'])\n",
    "        distincts.append(i['distinct'])\n",
    "    minValues = normalize([minValues], norm=\"max\")\n",
    "    maxValues = normalize([maxValues], norm=\"max\")\n",
    "    nullss = normalize([nullss], norm=\"max\")\n",
    "    distincts = normalize([distincts], norm=\"max\")\n",
    "\n",
    "    normalizes_column = {}\n",
    "    for i, (k, _) in enumerate(columns_all):\n",
    "        if(numerical[i] == 1):\n",
    "            normalizes_column[k] = np.array((numerical[i], minValues[0][i], maxValues[0][i], nullss[0][i], distincts[0][i]))\n",
    "        else:\n",
    "            normalizes_column[k] = np.array((numerical[i], 0, 1, nullss[0][i], distincts[0][i]))\n",
    "            \n",
    "    normalized_table = normalize([x[1:] for x in tables_all], axis=0)\n",
    "    normalizes_table = {}\n",
    "    for i in range(len(normalized_table)):\n",
    "        normalizes_table[tables_all[i][0]] = np.array((normalized_table[i][0],normalized_table[i][1]))\n",
    "    return (normalizes_column, normalizes_table)\n",
    "\n",
    "normalizes_column, normalizes_table = parse_table_metadata(\"imdb-metadata.json\")\n",
    "table_column_dict = {}\n",
    "for i in normalizes_column.keys():\n",
    "    table_column_dict[i.split(\".\")[1]] = i.split(\".\")[0]\n",
    "    \n",
    "table_column_dict # column -> table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_dim = 20\n",
    "\n",
    "data_dirs = ['../../imdb-6-16-2-3-48-one-round-1/', '../../imdb-6-16-2-3-48-one-round-missing-1/','../../imdb-6-16-2-3-48-one-round-2/', '../../imdb-6-16-2-3-48-one-round-missing-2/']\n",
    "dir_resources = [{'n_executor': 6, 'g_mem': 16, 'n_core':2, 'n_worker':3, 'parallelism': 48},{'n_executor': 6, 'g_mem': 16, 'n_core':2, 'n_worker':3, 'parallelism': 48},\\\n",
    "{'n_executor': 6, 'g_mem': 16, 'n_core':2, 'n_worker':3, 'parallelism': 48},{'n_executor': 6, 'g_mem': 16, 'n_core':2, 'n_worker':3, 'parallelism': 48},\\\n",
    "    {'n_executor': 6, 'g_mem': 16, 'n_core':2, 'n_worker':3, 'parallelism': 48},{'n_executor': 6, 'g_mem': 16, 'n_core':2, 'n_worker':3, 'parallelism': 48}]\n",
    "\n",
    "## Change to the data generated under your own environment\n",
    "\n",
    "query_ids = []\n",
    "plan_ids = []\n",
    "physical_plans = []\n",
    "times = []\n",
    "resources = []\n",
    "group_idxs = [] # record the dir that a data is from\n",
    "\n",
    "valid = [] # to check if the query is successfully executed, otherwise assign it a time of 1.5 * max\n",
    "for idx, data_dir in enumerate(data_dirs):\n",
    "    for file in os.listdir(data_dir):\n",
    "        csvreader = csv.reader(open(data_dir + file))\n",
    "        for row in csvreader:\n",
    "            if len(row) > 3:\n",
    "                valid.append((row[0], row[1]))\n",
    "\n",
    "for idx, data_dir in enumerate(data_dirs):\n",
    "    r = re.compile(r'.*?([a-zA-Z].*)')\n",
    "    for file in os.listdir(data_dir):\n",
    "        csvreader = csv.reader(open(data_dir + file))\n",
    "        for row in csvreader:\n",
    "            if len(row) > 3:\n",
    "                physical_plans.append(row[4])\n",
    "                query_ids.append(row[0])\n",
    "                plan_ids.append(row[1])\n",
    "                times.append(row[3])        \n",
    "                resources.append(dir_resources[idx])\n",
    "                group_idxs.append(idx)\n",
    "            else:\n",
    "                if not (row[0], row[1]) in valid:\n",
    "                    print(row[0], row[1])\n",
    "                    physical_plans.append(row[2])\n",
    "                    query_ids.append(row[0])\n",
    "                    plan_ids.append(row[1])\n",
    "                    times.append(-1)        \n",
    "                    resources.append(dir_resources[idx])\n",
    "                    group_idxs.append(idx)                   \n",
    "\n",
    "physical_plans = [[i for i in p.split(\"\\n\") if not i.startswith(\"==\")] for p in physical_plans]\n",
    "\n",
    "print(\"=== Number of valid physical plans: {}\".format(len(valid)))\n",
    "print(\"=== Number of physical plans: {}\".format(len(physical_plans)))\n",
    "\n",
    "zipped = list(zip(physical_plans, query_ids, plan_ids, times, resources, group_idxs))\n",
    "\n",
    "d = {}\n",
    "for i in zipped:\n",
    "    d[(i[1], i[2])] = i \n",
    "\n",
    "trains = []\n",
    "vals= []\n",
    "tests = []\n",
    "dir = 'imdb-sets/'\n",
    "\n",
    "with open(dir+'train.txt','r') as f:\n",
    "    for i in f.readlines():\n",
    "        q_id = i.strip().split(\" \")[0]\n",
    "        p_id = i.strip().split(\" \")[1]\n",
    "        trains.append(d[(q_id, p_id)])\n",
    "\n",
    "with open(dir+'val.txt','r') as f:\n",
    "    for i in f.readlines():\n",
    "        q_id = i.strip().split(\" \")[0]\n",
    "        p_id = i.strip().split(\" \")[1]\n",
    "        vals.append(d[(q_id, p_id)])\n",
    "        \n",
    "with open(dir+'test.txt','r') as f:\n",
    "    for i in f.readlines():\n",
    "        q_id = i.strip().split(\" \")[0]\n",
    "        p_id = i.strip().split(\" \")[1]\n",
    "        tests.append(d[(q_id, p_id)])\n",
    "\n",
    "zipped = trains + vals + tests \n",
    "physical_plans, query_ids, plan_ids, times, resources, group_idxs = list(zip(*zipped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, e in enumerate(times):\n",
    "    if e != -1:\n",
    "        print(i)\n",
    "        break\n",
    "print(\"== Example:\")\n",
    "print(\"Query Id: {}\".format(query_ids[i]))\n",
    "print(\"Plan Id: {}\".format(plan_ids[i]))\n",
    "print(\"Timeï¼š {}\".format(times[i]))\n",
    "print(\"Resources: {}\".format(resources[i]))\n",
    "print(\"Group Idx: {}\".format(group_idxs[i]))\n",
    "for e in physical_plans[i]:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_plan(operations): # operations: list of string\n",
    "    # split skeleton and details\n",
    "    skeleton = []\n",
    "    detail = []\n",
    "    flag = False\n",
    "    for o in operations:\n",
    "        if(len(o) > 0):\n",
    "            if(o[0] == \"(\"):\n",
    "                flag = True\n",
    "            if(flag == False):\n",
    "                skeleton.append(o)\n",
    "            else:\n",
    "                detail.append(o)\n",
    "    \n",
    "    return skeleton, detail\n",
    "\n",
    "class TreeNode:\n",
    "  def __init__(self, value, idx):\n",
    "    self.value = value \n",
    "    self.children = [] \n",
    "    self.idx = idx\n",
    " \n",
    "  def add_child(self, child_node):\n",
    "    self.children.append(child_node) \n",
    "\n",
    "  def __repr__(self, level=0):\n",
    "    ret = \"  \"*level+repr(self.value)+\"\\n\"\n",
    "    for child in self.children:\n",
    "        ret += child.__repr__(level+1)\n",
    "    return ret\n",
    "\n",
    "# build a tree from the strings  \n",
    "def parse_skeleton(skeleton):\n",
    "    nodes = []\n",
    "    for (i, o) in enumerate(skeleton):\n",
    "        # each line is a node, find its parent by back-tracking \":\"\n",
    "        level = 0\n",
    "        for (j, l) in enumerate(o):\n",
    "            if(l == \"-\"):\n",
    "                level = j\n",
    "                break\n",
    "        # construct node\n",
    "        name = re.sub(r\"[:* +-]\", '', o)\n",
    "        idx = int(name.split(\"(\")[-1][:-1]) - 1 # the index starts from 1, make it start from 0\n",
    "        name = name.split(\"(\")[0]\n",
    "        node = TreeNode(name, idx)\n",
    "        # find its parent\n",
    "        if(o.strip(\" :\").startswith(\"+-\")):\n",
    "            x = i-1\n",
    "            for x in range(i-1, 0, -1):\n",
    "                if(skeleton[x][level-1] != \":\"):\n",
    "                    break\n",
    "            nodes[x].add_child(node)          \n",
    "        elif(o.strip(\" :\").startswith(\"-\")):\n",
    "            nodes[-1].add_child(node)\n",
    "        # add node to nodes\n",
    "        nodes.append(node)\n",
    "    return nodes[0], nodes # the root of the tree and a list of all nodes\n",
    "\n",
    "# generate the structure matrix\n",
    "def gen_struct_matrix(nodes):\n",
    "    nodes_len = len(nodes)\n",
    "    matrix = np.zeros([nodes_len, nodes_len])\n",
    "    for node in nodes:\n",
    "        for child in node.children:\n",
    "            matrix[node.idx, child.idx] = 1 # is parent\n",
    "            matrix[child.idx, node.idx] = -1 # is child\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max([int(i) for i in times])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse physical plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "from collections import Counter, OrderedDict\n",
    "\n",
    "class Operation:\n",
    "    def __init__(self, s):\n",
    "        idx, operator = self.get_idx_operator(s)\n",
    "        self.idx = idx\n",
    "        self.operator = operator\n",
    "        self.auxi = self.get_auxi(s)\n",
    "    def __repr__(self):\n",
    "        return \"({}) {} \\n  {}\\n\".format(self.idx, self.operator, self.auxi)\n",
    "    def get_idx_operator(self, s):\n",
    "        idx = int(s[0].split(\")\")[0][1:])\n",
    "        operator = s[0].split(\")\")[1].split(\"[\")[0].strip()\n",
    "        return idx, operator\n",
    "    def get_auxi(self, s):\n",
    "        auxi_dict = {}\n",
    "        for i in s[1:]:\n",
    "            k = re.sub(r'[^a-zA-Z]', '', i.split(\":\")[0])\n",
    "            v = \":\".join(i.split(\":\")[1:]).strip() \n",
    "            v = re.sub(r'#[0-9]+', '', v.replace(\"[\",\"\").replace(\"]\",\"\")) # remove \"#xx\" and \"[]\"\n",
    "            if k!=\"Condition\": \n",
    "                v = v.split(\", \") # take the key words\n",
    "            else: v = [v]\n",
    "            auxi_dict[k] = v\n",
    "            \n",
    "        return auxi_dict\n",
    "    \n",
    "def gen_operator_vocab(operations):\n",
    "    operators = []\n",
    "    for o in operations:\n",
    "        operators += [i.operator for i in o]\n",
    "    counter = Counter(operators)\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "    v = vocab(ordered_dict, specials=[\"<unk>\"])\n",
    "    v.set_default_index(v[\"<unk>\"])\n",
    "    return v\n",
    "\n",
    "def gen_detail_vocab(operations):\n",
    "    words = []\n",
    "    for o in operations:\n",
    "        for s in o:\n",
    "            for k,v in s.auxi.items():\n",
    "                words.append(k)\n",
    "                for x in v:\n",
    "                    words.append(x)\n",
    "    counter = Counter(words)\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "    v = vocab(ordered_dict, specials=[\"<unk>\"])\n",
    "    v.set_default_index(v[\"<unk>\"])\n",
    "    return v\n",
    "\n",
    "def gen_all_vocab(operations):\n",
    "    words = []\n",
    "    for o in operations:\n",
    "        words += [i.operator for i in o]\n",
    "        for s in o:\n",
    "            for k,v in s.auxi.items():\n",
    "                words.append(k)\n",
    "                for x in v:\n",
    "                    words.append(x)\n",
    "    counter = Counter(words)\n",
    "    sorted_by_freq_tuples = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
    "    v = vocab(ordered_dict, specials=[\"<unk>\"])\n",
    "    v.set_default_index(v[\"<unk>\"])\n",
    "    return v\n",
    "\n",
    "# parse details\n",
    "def parse_detail(detail):\n",
    "    operations = [[detail[0]]]\n",
    "    parsed_operations = []\n",
    "    for line in detail[1:]:\n",
    "        if(line.startswith(\"(\")):\n",
    "            operations.append([line])\n",
    "        else:\n",
    "            last = operations[-1]\n",
    "            operations[:-1].append(last.append(line))\n",
    "    for operation in operations:\n",
    "        parsed_operations.append(Operation(operation))\n",
    "    return parsed_operations # return a list of operations, each of which contains operator and auxiliary info\n",
    "\n",
    "# operation to words\n",
    "def op_to_vector(operation, v):\n",
    "    words = [v.lookup_indices([operation.operator])]\n",
    "    for i in operation.auxi.keys():\n",
    "        words.append(v.lookup_indices([str(i)]))\n",
    "        words.append(v.lookup_indices(operation.auxi[i]))\n",
    "\n",
    "    words = [item for sublist in words for item in sublist]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test all\n",
    "# take the information of each operator (not formatted)\n",
    "structures = []\n",
    "details = []\n",
    "skeleton_lens = []\n",
    "for plan in physical_plans:\n",
    "    skeleton, detail = split_plan(plan)\n",
    "    _, nodes = parse_skeleton(skeleton)\n",
    "    skeleton_lens.append(len(skeleton))\n",
    "    structures.append(gen_struct_matrix(nodes))\n",
    "    details.append(parse_detail(detail))\n",
    "print(len(structures))\n",
    "print(len(details))\n",
    "\n",
    "v = gen_operator_vocab(details)\n",
    "all_v = gen_all_vocab(details)\n",
    "print(\"operator vocab length: {}\".format(len(v)))\n",
    "print(v.get_itos())\n",
    "print(\"all vocab length: {}\".format(len(all_v)))\n",
    "\n",
    "from copy import deepcopy\n",
    "details_copy =deepcopy(details)\n",
    "\n",
    "print(min(skeleton_lens))\n",
    "print(max(skeleton_lens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract key information and reformat it\n",
    "# key information: table, column, predicate\n",
    "class Condition: \n",
    "    def __init__(self, column, operator, operand):\n",
    "        self.column = column\n",
    "        self.operator = operator\n",
    "        self.operand = operand\n",
    "    def __repr__(self):\n",
    "        return \"Condition <{} {} {}>\".format(self.column, self.operator, self.operand)\n",
    "\n",
    "def reformat_scanparquet(operation):\n",
    "    auxi = operation.auxi\n",
    "    table = auxi[\"Location\"][0].split(\"/\")[-1]\n",
    "    columns = auxi[\"Output\"]\n",
    "    columns_w_table = [table+\".\"+c for c in columns]\n",
    "    new_auxi = {}\n",
    "    new_auxi[\"Table\"] = [table]\n",
    "    new_auxi[\"Columns\"] = columns_w_table\n",
    "    conditions = []\n",
    "    if auxi.get(\"PushedFilters\"):\n",
    "        for i in auxi[\"PushedFilters\"]:\n",
    "            if len(i.split(\"(\")) > 1:\n",
    "                operator = i.split(\"(\")[0]\n",
    "                objects = i.split(\"(\")[1][:-1].split(\",\")\n",
    "                o = table+\".\"+objects[0]\n",
    "                if(len(objects) > 1):\n",
    "                    operand = objects[1]\n",
    "                    if(operand.isnumeric()):\n",
    "                        operand = float(operand)\n",
    "                    elif(operand in columns):\n",
    "                        operand = table+\".\"+operand\n",
    "                    else:\n",
    "                        pass\n",
    "                        # print(\"unsupported operand: {}\".format(operand))\n",
    "                else: \n",
    "                    operand = ''\n",
    "                conditions.append(Condition(o, operator, operand))\n",
    "        # new_auxi[\"Condition\"] = conditions\n",
    "        new_auxi[\"Condition\"] = []\n",
    "        for i in auxi[\"PushedFilters\"]:\n",
    "            new_auxi[\"Condition\"] += i.split(\", \")\n",
    "    operation.auxi = new_auxi\n",
    "    return operation\n",
    "\n",
    "def reformat_scancsv(operation):\n",
    "    auxi = operation.auxi\n",
    "    table = auxi[\"Location\"][0].split(\"/\")[-1].split(\".\")[0]\n",
    "    columns = [i.replace(\"L\",\"\") for i in auxi[\"Output\"]]\n",
    "    columns_w_table = [table+\".\"+c for c in columns]\n",
    "    new_auxi = {}\n",
    "    new_auxi[\"Table\"] = [table]\n",
    "    new_auxi[\"Columns\"] = columns_w_table\n",
    "    conditions = []\n",
    "    \n",
    "    if auxi.get('PushedFilters'):\n",
    "        # hard code for wrongly split\n",
    "        pushedfilters = []\n",
    "        i = 0\n",
    "        while i < len(auxi[\"PushedFilters\"]):\n",
    "            e = auxi[\"PushedFilters\"][i]\n",
    "            if e.startswith(\"In\"): \n",
    "                pushedfilters.append(e+\",\"+auxi[\"PushedFilters\"][i+1])\n",
    "                i += 2\n",
    "            else: \n",
    "                pushedfilters.append(e)\n",
    "                i += 1\n",
    "                \n",
    "        # print(pushedfilters)\n",
    "        for i in pushedfilters:\n",
    "            operator = i.split(\"(\")[0]\n",
    "            objects = i.split(\"(\")[1][:-1].split(\",\")\n",
    "            o = table+\".\"+objects[0]\n",
    "            if(len(objects) > 1):\n",
    "                operand = objects[1]\n",
    "                if(operand.isnumeric()):\n",
    "                    operand = float(operand)\n",
    "                elif(operand in columns):\n",
    "                    operand = table+\".\"+operand\n",
    "                else:\n",
    "                    pass\n",
    "                    # print(\"unsupported operand: {}\".format(operand))\n",
    "            else: \n",
    "                operand = ''\n",
    "            conditions.append(Condition(o, operator, operand))\n",
    "        # new_auxi[\"Condition\"] = conditions\n",
    "        new_auxi[\"Condition\"] = []\n",
    "        for i in auxi[\"PushedFilters\"]:\n",
    "            new_auxi[\"Condition\"] += i.split(\", \")\n",
    "    operation.auxi = new_auxi\n",
    "    return operation\n",
    "\n",
    "def reformat_logicalrelation(operation, parents):\n",
    "    auxi = operation.auxi\n",
    "    columns = [i for i in auxi['Arguments'] if not i in ['parquet', 'true', 'false']]\n",
    "    columns = [i[:-1] for i in columns if i.endswith(\"L\")]\n",
    "    tables = [table_column_dict[i] for i in columns]\n",
    "    auxi['Table'] = list(dict.fromkeys(tables))\n",
    "    auxi['Columns'] = [table_column_dict[i]+'.'+i for i in columns]\n",
    "    return operation\n",
    "\n",
    "def reformat_filter(operation, parents):\n",
    "    parent = parents[0]\n",
    "    table = parent.auxi[\"Table\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = table\n",
    "    if auxi.get('Input'):\n",
    "        columns = [i.replace(\"L\",\"\") for i in auxi[\"Input\"]]\n",
    "        auxi['Columns'] = [table[0]+\".\"+i for i in columns]\n",
    "    else:\n",
    "        auxi['Columns'] = parent.auxi[\"Columns\"]\n",
    "    # todo parse conditions\n",
    "    if auxi.get(\"Condition\"):\n",
    "        auxi['Condition'] = [i.replace(\"L\", \"\") for i in auxi['Condition']]\n",
    "    else:\n",
    "        auxi['Condition'] = []\n",
    "    return operation\n",
    "\n",
    "def reformat_project(operation, parents): \n",
    "    # project acts like glue - it records the tables and comlumns of the ancestors \n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = []\n",
    "    output =  auxi[\"Output\"] if auxi.get(\"Output\") else auxi[\"Arguments\"]\n",
    "    for i in output:\n",
    "        for j in columns:\n",
    "            if i.replace(\"L\", \"\") in j:\n",
    "                auxi['Columns'].append(j)\n",
    "                break\n",
    "    return operation\n",
    "\n",
    "def reformat_exchange(operation, parents):\n",
    "    parent = parents[0]\n",
    "    table = parent.auxi[\"Table\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = table\n",
    "    auxi['Columns'] = [table[0]+\".\"+i.strip('L') for i in auxi[\"Input\"]]\n",
    "    return operation\n",
    "\n",
    "def reformat_reusedexchange(operation, parents):\n",
    "    auxi = operation.auxi\n",
    "    columns = [i.strip('L') for i in auxi[\"Output\"]]\n",
    "    tables = [table_column_dict[i] for i in columns]\n",
    "    auxi['Table'] = list(dict.fromkeys(tables))\n",
    "    auxi['Columns'] = [table_column_dict[i]+'.'+i for i in columns]\n",
    "    return operation\n",
    "\n",
    "def reformat_hashagg(operation, parents):\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = columns\n",
    "    auxi['Condition'] = [i.replace(\"L\", \"\") for i in [\" AND \".join([i for i in \\\n",
    "        auxi['Functions'] + auxi['AggregateAttributes']])]]\n",
    "    return operation    \n",
    "\n",
    "def reformat_sortagg(operation, parents):\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = columns\n",
    "    auxi['Condition'] = [i.replace(\"L\", \"\") for i in [\" AND \".join([i for i in \\\n",
    "        auxi['Functions'] + auxi['AggregateAttributes']])]]\n",
    "    return operation      \n",
    "\n",
    "def reformat_agg(operation, parents):\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = columns\n",
    "    auxi['Condition'] = []\n",
    "    return operation  \n",
    "\n",
    "def reformat_sort(operation, parents):\n",
    "    # print(parents)\n",
    "    parent = parents[0]\n",
    "    table = parent.auxi[\"Table\"]\n",
    "    auxi = operation.auxi\n",
    "    columns = [i.replace(\"L\",\"\") for i in auxi[\"Input\"]]\n",
    "    auxi[\"Table\"] = table\n",
    "    auxi['Columns'] = [table[0]+\".\"+i for i in columns]\n",
    "    auxi[\"Condition\"] = [\" AND \".join(auxi['Arguments'])]\n",
    "    return operation  \n",
    "\n",
    "def reformat_smjoin(operation, parents):\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = []\n",
    "    for i in auxi[\"Leftkeys\"] + auxi[\"Rightkeys\"]:\n",
    "        for j in columns:\n",
    "            if i.replace(\"L\",\"\") in j:\n",
    "                auxi['Columns'].append(j)\n",
    "    return operation\n",
    "\n",
    "def reformat_join(operation, parents):  \n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = []\n",
    "    c = [i.replace(\"L\",\"\").replace(\"(\",\"\").replace(\")\",\"\") for i in auxi['Arguments'][1].split(\" = \")] # todo: currently hardcoded\n",
    "    for i in c:\n",
    "        for j in columns:\n",
    "            if i in j:\n",
    "                auxi['Columns'].append(j)\n",
    "                break\n",
    "    return operation \n",
    "    \n",
    "def reformat_bchjoin(operation, parents):\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = []\n",
    "    for i in auxi[\"Leftkeys\"] + auxi[\"Rightkeys\"]:\n",
    "        i = i.replace(\"L\", \"\")\n",
    "        for j in columns:\n",
    "            if i in j:\n",
    "                auxi['Columns'].append(j)\n",
    "                break\n",
    "    return operation \n",
    "\n",
    "def reformat_schjoin(operation, parents):\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = []\n",
    "    for i in auxi[\"Leftkeys\"] + auxi[\"Rightkeys\"]:\n",
    "        i = i.replace(\"L\", \"\")\n",
    "        for j in columns:\n",
    "            if i in j:\n",
    "                auxi['Columns'].append(j)\n",
    "                break\n",
    "    return operation \n",
    "\n",
    "def reformat_cartesianproduct(operation, parents):\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = []\n",
    "    c = [i.strip() for i in parent.auxi[\"Columns\"][0].replace(\"L\",\"\").replace(\"(\",\"\").replace(\")\",\"\").split(\"=\")]\n",
    "    for i in c:\n",
    "        for j in columns:\n",
    "            if i in j:\n",
    "                auxi['Columns'].append(j)\n",
    "                break\n",
    "    return operation\n",
    "\n",
    "def reformat_bcexchange(operation, parents):\n",
    "    tables = []\n",
    "    columns = []\n",
    "    for parent in parents:\n",
    "        tables += parent.auxi[\"Table\"]\n",
    "        columns += parent.auxi[\"Columns\"]\n",
    "    auxi = operation.auxi\n",
    "    auxi[\"Table\"] = tables\n",
    "    auxi['Columns'] = []\n",
    "    for i in auxi[\"Input\"]:\n",
    "        for j in columns:\n",
    "            if i in j:\n",
    "                auxi['Columns'].append(j)\n",
    "                break\n",
    "    return operation\n",
    "\n",
    "\n",
    "def reformat(operation, structure):\n",
    "    parent_indices = [i + 1 for i, x in enumerate(structure[operation.idx - 1]) if x == 1]\n",
    "    parent_operations = [operation_dict[i] for i in parent_indices]\n",
    "\n",
    "    if(operation.operator) == \"Scan csv\":\n",
    "        reformat_scancsv(operation)\n",
    "    elif(operation.operator) == \"Scan parquet\":\n",
    "        reformat_scanparquet(operation)\n",
    "    elif(operation.operator) == \"Filter\":\n",
    "        reformat_filter(operation, parent_operations)\n",
    "    elif(operation.operator) == \"Project\":\n",
    "        reformat_project(operation, parent_operations)\n",
    "    elif(operation.operator) == \"Exchange\":\n",
    "        reformat_exchange(operation, parent_operations)\n",
    "    elif(operation.operator) == \"ReusedExchange\":\n",
    "        reformat_reusedexchange(operation, parent_operations)\n",
    "    elif(operation.operator) == \"HashAggregate\":\n",
    "        reformat_hashagg(operation, parent_operations)\n",
    "    elif(operation.operator) == \"SortAggregate\":\n",
    "        reformat_sortagg(operation, parent_operations)\n",
    "    elif(operation.operator) == \"Aggregate\":\n",
    "        reformat_agg(operation, parent_operations)\n",
    "    elif(operation.operator) == \"Sort\":\n",
    "        reformat_sort(operation, parent_operations)\n",
    "    elif(operation.operator) == \"SortMergeJoin\":\n",
    "        reformat_smjoin(operation, parent_operations)\n",
    "    elif(operation.operator) == \"BroadcastExchange\":\n",
    "        reformat_bcexchange(operation, parent_operations)\n",
    "    elif(operation.operator) == \"BroadcastHashJoin\":\n",
    "        reformat_bchjoin(operation, parent_operations)\n",
    "    elif(operation.operator) == \"ShuffledHashJoin\":\n",
    "        reformat_schjoin(operation, parent_operations)\n",
    "    elif(operation.operator) == \"LogicalRelation\":\n",
    "        reformat_logicalrelation(operation, parent_operations)\n",
    "    elif(operation.operator) == \"Join\":\n",
    "        reformat_join(operation, parent_operations)\n",
    "    elif(operation.operator) == \"CartesianProduct\":\n",
    "        reformat_cartesianproduct(operation, parent_operations)       \n",
    "    else:\n",
    "        print(\"Unseen operation: {}\".format(operation.operator))\n",
    "        print(operation)\n",
    "        \n",
    "\n",
    "details =deepcopy(details_copy)\n",
    "for i, o in enumerate(details):\n",
    "    operation_dict = {}\n",
    "    for d in o:\n",
    "        operation_dict[d.idx] = d # cannot combine the two, otherwise cannot find parents sometimes\n",
    "    for d in o:\n",
    "        reformat(d, structures[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "print(len(details))\n",
    "for d in details:\n",
    "    for i in d:\n",
    "        if i.auxi.get('Columns'):\n",
    "            c = i.auxi['Columns']\n",
    "            for j in c:\n",
    "                if 'L' in j:\n",
    "                    print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec for predicate keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"condition_word2vec_imdb.model\"\n",
    "\n",
    "from os.path import exists\n",
    "\n",
    "# build a word2vec model for keywords in a predicate\n",
    "def gen_condition_sentences(queries):\n",
    "    trimmed_conditionss  = []\n",
    "    for q in queries:\n",
    "        conditions = [i.auxi.get(\"Condition\") for i in q]\n",
    "        conditions = [' '.join(i).replace(\"AND\",\"\").replace(\"OR\", \"\").replace(\"(\", \" \").replace(\")\", \" \") for i in conditions if i!=None]\n",
    "        conditions = [re.sub(' +', ' ', i).split(\" \") for i in conditions]\n",
    "        for c in conditions:\n",
    "            for i in c:\n",
    "                if(i == \"\"): c.remove(i)\n",
    "        trimmed_conditionss +=  conditions\n",
    "    return trimmed_conditionss\n",
    "\n",
    "sentences = gen_condition_sentences(details[:int(0.8* len(details))])\n",
    "print(len(sentences))\n",
    "print(len(details))\n",
    "# print([i for i in details])\n",
    "print(sentences[0])\n",
    "\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=sentences, vector_size=20, window=5, min_count=10, workers=4)\n",
    "model.save(model_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "ft = FastText(sentences, min_count=1, vector_size=condition_dim)\n",
    "\n",
    "from fse import Average, IndexedList\n",
    "from fse.models import uSIF\n",
    "sif_model = uSIF(ft)\n",
    "sif_model.train(IndexedList(sentences))\n",
    "\n",
    "sif_model.save(\"usif_imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot to encode operators, tables, and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index the keywords\n",
    "\n",
    "def onehot(l):\n",
    "    length = len(l)\n",
    "    arr = np.zeros([length, length])\n",
    "    for i in range(length):\n",
    "        arr[i][i] = 1\n",
    "    d = {}\n",
    "    for idx, i in enumerate(l):\n",
    "        d[i] = arr[idx]\n",
    "    return d\n",
    "    \n",
    "tables = []\n",
    "columns = []\n",
    "operations = []\n",
    "predicates = []\n",
    "for o in details:\n",
    "    for operation in o:\n",
    "        operations += [operation.operator]\n",
    "        if (operation.operator) == \"Scan csv\" or (operation.operator) == \"Scan parquet\":\n",
    "            columns = columns + operation.auxi[\"Columns\"]\n",
    "            tables = tables + operation.auxi[\"Table\"]\n",
    "        else:\n",
    "            columns = columns + operation.auxi[\"Columns\"]\n",
    "\n",
    "columns = (list(set(columns)))\n",
    "tables = (list(set(tables)))\n",
    "operations = (list(set(operations)))\n",
    "\n",
    "print(len(columns))\n",
    "print(len(tables)) \n",
    "print(len(operations))\n",
    "\n",
    "print(columns)\n",
    "\n",
    "column_onehot = onehot(columns)\n",
    "table_onehot = onehot(tables)\n",
    "operation_onehot = onehot(operations)\n",
    "table_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results \n",
    "import pickle\n",
    "with open('column_onehot_imdb.pkl', 'wb') as f:\n",
    "    pickle.dump(column_onehot, f)\n",
    "with open('table_onehot_imdb.pkl', 'wb') as f:\n",
    "    pickle.dump(table_onehot, f)\n",
    "with open('operation_onehot_imdb.pkl', 'wb') as f:\n",
    "    pickle.dump(operation_onehot, f)\n",
    "\n",
    "print(len(pickle.load(open('table_onehot_imdb.pkl', 'rb'))))\n",
    "print(len(pickle.load(open('operation_onehot_imdb.pkl', 'rb'))))\n",
    "print(len(column_onehot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizes_column, normalizes_table = parse_table_metadata(\"imdb-metadata.json\")\n",
    "\n",
    "print(len(normalizes_table))\n",
    "print(len(normalizes_column))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode the entire operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "column_onehot = pickle.load(open('column_onehot_imdb.pkl', 'rb'))\n",
    "table_onehot = pickle.load(open('table_onehot_imdb.pkl', 'rb'))\n",
    "operation_onehot = pickle.load(open('operation_onehot_imdb.pkl', 'rb'))\n",
    "condition_model = Word2Vec.load('condition_word2vec_imdb.model')\n",
    "normalizes_column, normalizes_table = parse_table_metadata(\"imdb-metadata.json\")\n",
    "# encode the key information of each operation\n",
    "def encode_operation(operation, operation_onehot, table_onehot, column_onehot, condition_model, normalizes_column, normalizes_table):\n",
    "    auxi = operation.auxi\n",
    "    if (auxi.get('Table')):\n",
    "        table_v = np.concatenate((table_onehot[auxi[\"Table\"][0]], normalizes_table[auxi[\"Table\"][0]]))\n",
    "    else:\n",
    "        table_v = np.zeros(len(table_onehot)+len(list(normalizes_table.values())[0]))\n",
    "    normalizes_column_size = len(list(normalizes_column.values())[0])\n",
    "    try:\n",
    "        column_v = [np.concatenate((column_onehot[i], normalizes_column.get(i, np.zeros(normalizes_column_size)))) for i in auxi[\"Columns\"]]\n",
    "        column_v = np.concatenate(column_v, axis=0)\n",
    "        operator_v = operation_onehot[operation.operator]\n",
    "    except:\n",
    "        column_v = np.zeros(list(column_onehot.values())[0].size)\n",
    "        operator_v = np.zeros(list(operation_onehot.values())[0].size)\n",
    "        print(\"Dealing with unseen/problematic operator: {}\".format(operation))\n",
    "\n",
    "    if(auxi.get(\"Condition\")):\n",
    "        condition = auxi[\"Condition\"]\n",
    "        condition_v = sif_model.infer([(condition, 0)])\n",
    "\n",
    "    else:\n",
    "        condition_v = np.zeros(condition_dim)\n",
    "    return operator_v, table_v, column_v, condition_v\n",
    "\n",
    "operator_v, table_v, column_v, condition_v = encode_operation(details[0][0], operation_onehot, table_onehot, column_onehot, condition_model, normalizes_column, normalizes_table)\n",
    "print(operator_v.shape)\n",
    "print(table_v.shape)\n",
    "print(column_v.shape)\n",
    "print(condition_v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# details to vectors\n",
    "encoded = []\n",
    "max_seq_len = max([i.shape[0] for i in structures])\n",
    "print(max_seq_len)\n",
    "for query, structure in zip(details, structures):\n",
    "    q = []\n",
    "    for i, operation in enumerate(query):\n",
    "        operator_v, table_v, column_v, condition_v = encode_operation(operation, operation_onehot, table_onehot, column_onehot, condition_model,normalizes_column, normalizes_table)\n",
    "        connectivity = structure[i]\n",
    "        connectivity = np.pad(connectivity, (0,max_seq_len - len(connectivity)), \"constant\", constant_values = (0))\n",
    "        vs = {\"operator\": operator_v, \"column\": column_v, \"condition\": condition_v, \"table\": table_v, \"structure\": connectivity}\n",
    "        q.append(vs)\n",
    "    encoded.append(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 padding for each operator\n",
    "column_len = []\n",
    "for i in encoded:\n",
    "    for j in i:\n",
    "        column_len.append(j[\"column\"].shape)\n",
    "# max_column_len = max(column_len)[0]\n",
    "max_column_len=100\n",
    "print(max_column_len)\n",
    "vs = []\n",
    "for i, plan_id, query_id, t, r, g in zip(encoded, plan_ids, query_ids, times, resources, group_idxs):\n",
    "    v = []\n",
    "    for j in i:\n",
    "        j[\"column\"] = np.pad(j[\"column\"], (0,max_column_len - len(j[\"column\"])), \"constant\", constant_values = (0))\n",
    "        # print(j[\"condition\"])\n",
    "        v.append(np.concatenate((j[\"operator\"], j[\"table\"], j[\"column\"], j[\"condition\"], j[\"structure\"]), axis=None))\n",
    "        # print(j[\"operator\"].shape)\n",
    "        # print(j[\"table\"].shape)\n",
    "        # print(j[\"column\"].shape)\n",
    "        # print(j[\"condition\"].shape)\n",
    "        # print(j[\"structure\"].shape)\n",
    "    v = np.vstack(v)\n",
    "    print(v.shape)\n",
    "    vs.append((plan_id, query_id, v, t, r, g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(j[\"table\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the parsed plans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# d = {}\n",
    "# vs2 = []\n",
    "# for plan_id, query_id, v, t in vs:\n",
    "#     v = torch.tensor(v, dtype=torch.float32)\n",
    "#     d[(int(query_id), int(plan_id))] = (v, float(t))\n",
    "#     vs2.append(v)\n",
    "# vs2 = np.array(vs2)\n",
    "# np.save(\"encoded_plans_job_2.npy\", vs2)\n",
    "# with open('encoded.pkl_2', 'wb') as f:\n",
    "#     pickle.dump(d, f)\n",
    "\n",
    "#     # d: {(qId, pId) -> (v, t)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "d = {}\n",
    "vs2 = []\n",
    "for i, (plan_id, query_id, v, t, r, g) in enumerate(vs):\n",
    "    v = torch.tensor(v, dtype=torch.float32)\n",
    "    d[(query_id, plan_id, g, i)] = (v, float(t), r)\n",
    "    vs2.append(v)\n",
    "vs2 = np.array(vs2)\n",
    "np.save(\"encoded_plans_imdb.npy\", vs2)\n",
    "with open('encoded_imdb.pkl', 'wb') as f:\n",
    "    pickle.dump(d, f)\n",
    "\n",
    "    # d: {(qId, pId, group) -> (v, t, resources)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "96d7d9c4850cad55e90ffbd2129f1103be678d0d036c41e3b7c74e0c2bc67127"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
